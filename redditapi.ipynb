{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "spacy.info(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I have a dilemma, embarrassing!\n",
      "My husband sort of hurt my feelings he said there is something I do everyday that bothers him. I asked what and he says he doesnâ€™t understand why I wash my hands after pooping before going into the shower. Yes, I leave the bathroom to wash my hands because the sink is not in the same room as the toilet and shower. Iâ€™ve done this nearly two decades. I thought washing my hands before showering was sanitary. Iâ€™m embarrassed ask four other people and no one does this. How do I stop this habit? \n",
      "\n",
      "Edit: Thank you to the people who understood how I feel in this situation learned something new not to be ashamed or embarrassed. Iâ€™ll just keep washing my hands. Have a good evening. \n",
      "182\n"
     ]
    }
   ],
   "source": [
    "import praw\n",
    "\n",
    "client_id = \"cfgRNdcf6Vw7PF9mBKrWKA\"\n",
    "client_secret = \"k8YD6GloeBp5uSvV0VHOMknpyjjIrA\"\n",
    "user_agent = \"conheo\"\n",
    "\n",
    "reddit = praw.Reddit(\n",
    "    client_id=client_id,\n",
    "    client_secret=client_secret,\n",
    "    user_agent=user_agent\n",
    ")\n",
    "\n",
    "subreddit = reddit.subreddit(\"writing\")\n",
    "u=0\n",
    "for submission in subreddit.hot(limit=10):  # Sá»­ dá»¥ng vÃ²ng láº·p Ä‘á»“ng bá»™\n",
    "    print(submission.title)\n",
    "    print(submission.selftext)\n",
    "    print(submission.ups )\n",
    "    submission.comments.replace_more(limit=0)  # Táº£i toÃ n bá»™ comment\n",
    "    \n",
    "    # for comment in submission.comments.list():  # Láº¥y danh sÃ¡ch comment\n",
    "    #     print(f\"cmt {u} vote {comment.ups }\",comment.body)\n",
    "    #     u+=1\n",
    "    break  # Chá»‰ láº¥y 1 bÃ i viáº¿t Ä‘áº§u tiÃªn\n",
    "# ÄÃ³ng káº¿t ná»‘i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Comment(id='mh1ws9t')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission.comments.list()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mgensim\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m KeyedVectors\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Táº£i mÃ´ hÃ¬nh Word2Vec pretrained (Google News)\u001b[39;00m\n\u001b[0;32m      4\u001b[0m model \u001b[38;5;241m=\u001b[39m KeyedVectors\u001b[38;5;241m.\u001b[39mload_word2vec_format(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGoogleNews-vectors-negative300.bin\u001b[39m\u001b[38;5;124m'\u001b[39m, binary\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\caoth\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\gensim\\__init__.py:11\u001b[0m\n\u001b[0;32m      7\u001b[0m __version__ \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m4.3.3\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlogging\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mgensim\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m parsing, corpora, matutils, interfaces, models, similarities, utils  \u001b[38;5;66;03m# noqa:F401\u001b[39;00m\n\u001b[0;32m     14\u001b[0m logger \u001b[38;5;241m=\u001b[39m logging\u001b[38;5;241m.\u001b[39mgetLogger(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgensim\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m logger\u001b[38;5;241m.\u001b[39mhandlers:  \u001b[38;5;66;03m# To ensure reload() doesn't add another one\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\caoth\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\gensim\\corpora\\__init__.py:6\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;124;03mThis package contains implementations of various streaming corpus I/O format.\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# bring corpus classes directly into package namespace, to save some typing\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mindexedcorpus\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m IndexedCorpus  \u001b[38;5;66;03m# noqa:F401 must appear before the other classes\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmmcorpus\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m MmCorpus  \u001b[38;5;66;03m# noqa:F401\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbleicorpus\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m BleiCorpus  \u001b[38;5;66;03m# noqa:F401\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\caoth\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\gensim\\corpora\\indexedcorpus.py:14\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlogging\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\n\u001b[1;32m---> 14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mgensim\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m interfaces, utils\n\u001b[0;32m     16\u001b[0m logger \u001b[38;5;241m=\u001b[39m logging\u001b[38;5;241m.\u001b[39mgetLogger(\u001b[38;5;18m__name__\u001b[39m)\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mIndexedCorpus\u001b[39;00m(interfaces\u001b[38;5;241m.\u001b[39mCorpusABC):\n",
      "File \u001b[1;32mc:\\Users\\caoth\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\gensim\\interfaces.py:19\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;124;03m\"\"\"Basic interfaces used across the whole Gensim package.\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \n\u001b[0;32m      9\u001b[0m \u001b[38;5;124;03mThese interfaces are used for building corpora, model transformation and similarity queries.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     14\u001b[0m \n\u001b[0;32m     15\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlogging\u001b[39;00m\n\u001b[1;32m---> 19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mgensim\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m utils, matutils\n\u001b[0;32m     22\u001b[0m logger \u001b[38;5;241m=\u001b[39m logging\u001b[38;5;241m.\u001b[39mgetLogger(\u001b[38;5;18m__name__\u001b[39m)\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mCorpusABC\u001b[39;00m(utils\u001b[38;5;241m.\u001b[39mSaveLoad):\n",
      "File \u001b[1;32mc:\\Users\\caoth\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\gensim\\matutils.py:1034\u001b[0m\n\u001b[0;32m   1029\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;241m1.\u001b[39m \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;28mlen\u001b[39m(set1 \u001b[38;5;241m&\u001b[39m set2)) \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mfloat\u001b[39m(union_cardinality)\n\u001b[0;32m   1032\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1033\u001b[0m     \u001b[38;5;66;03m# try to load fast, cythonized code if possible\u001b[39;00m\n\u001b[1;32m-> 1034\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mgensim\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_matutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m logsumexp, mean_absolute_difference, dirichlet_expectation\n\u001b[0;32m   1036\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[0;32m   1037\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mlogsumexp\u001b[39m(x):\n",
      "File \u001b[1;32mc:\\Users\\caoth\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\gensim\\_matutils.pyx:1\u001b[0m, in \u001b[0;36minit gensim._matutils\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject"
     ]
    }
   ],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "\n",
    "# Táº£i mÃ´ hÃ¬nh Word2Vec pretrained (Google News)\n",
    "model = KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)\n",
    "\n",
    "# Demo: TÃ¬m vector vÃ  tá»« tÆ°Æ¡ng tá»±\n",
    "word = \"king\"\n",
    "vector = model[word]  # Vector cá»§a tá»« \"king\"\n",
    "similar_words = model.most_similar(word, topn=5)  # 5 tá»« gáº§n nháº¥t\n",
    "\n",
    "print(f\"Vector cá»§a '{word}': {vector[:5]}... (kÃ­ch thÆ°á»›c: {len(vector)})\")\n",
    "print(f\"CÃ¡c tá»« tÆ°Æ¡ng tá»± '{word}': {similar_words}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting class of d5: B\n",
      "Probability of d6 in each class: [[0.29175335 0.70824665]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "# Dá»¯ liá»‡u huáº¥n luyá»‡n\n",
    "d1 = [2, 1, 1, 0, 0, 0, 0, 0, 0]\n",
    "d2 = [1, 1, 0, 1, 1, 0, 0, 0, 0]\n",
    "d3 = [0, 1, 0, 0, 1, 1, 0, 0, 0]\n",
    "d4 = [0, 1, 0, 0, 0, 0, 1, 1, 1]\n",
    "\n",
    "train_data = np.array([d1, d2, d3, d4])\n",
    "labels = np.array(['B', 'B', 'B', 'N'])  # NhÃ£n cho dá»¯ liá»‡u huáº¥n luyá»‡n\n",
    "\n",
    "# Dá»¯ liá»‡u kiá»ƒm tra\n",
    "d5 = np.array([[2, 0, 0, 1, 0, 0, 0, 1, 0]])  # Cáº§n Ä‘á»ƒ trong [[]] vÃ¬ sklearn yÃªu cáº§u dáº¡ng (n_samples, n_features)\n",
    "d6 = np.array([[0, 1, 0, 0, 0, 0, 0, 1, 1]])\n",
    "\n",
    "# Khá»Ÿi táº¡o vÃ  huáº¥n luyá»‡n mÃ´ hÃ¬nh\n",
    "model = MultinomialNB()\n",
    "model.fit(train_data, labels)\n",
    "\n",
    "# Dá»± Ä‘oÃ¡n\n",
    "print(\"Predicting class of d5:\", model.predict(d5)[0])\n",
    "print(\"Probability of d6 in each class:\", model.predict_proba(d6))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['alt.atheism', 'comp.graphics', 'comp.os.ms-windows.misc', 'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware', 'comp.windows.x', 'misc.forsale', 'rec.autos', 'rec.motorcycles', 'rec.sport.baseball', 'rec.sport.hockey', 'sci.crypt', 'sci.electronics', 'sci.med', 'sci.space', 'soc.religion.christian', 'talk.politics.guns', 'talk.politics.mideast', 'talk.politics.misc', 'talk.religion.misc']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "data = fetch_20newsgroups(subset='train')\n",
    "print(data.target_names)  # CÃ¡c nhÃ£n (categories)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¯ Äá»™ chÃ­nh xÃ¡c trÃªn táº­p test: 0.9462\n",
      "ğŸ“Œ VÄƒn báº£n: NASA is planning a new space mission to Mars.\n",
      "ğŸ‘‰ Chá»§ Ä‘á» dá»± Ä‘oÃ¡n: sci.space\n",
      "\n",
      "ğŸ“Œ VÄƒn báº£n: The graphics card performance has significantly improved in recent years.\n",
      "ğŸ‘‰ Chá»§ Ä‘á» dá»± Ä‘oÃ¡n: comp.graphics\n",
      "\n",
      "ğŸ“Œ VÄƒn báº£n: The baseball game last night was thrilling!\n",
      "ğŸ‘‰ Chá»§ Ä‘á» dá»± Ä‘oÃ¡n: rec.sport.baseball\n",
      "\n",
      "ğŸ“Œ VÄƒn báº£n: The new gun control laws have sparked heated debates.\n",
      "ğŸ‘‰ Chá»§ Ä‘á» dá»± Ä‘oÃ¡n: talk.politics.guns\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn import metrics\n",
    "\n",
    "# 1ï¸âƒ£ Táº£i dá»¯ liá»‡u (chá»‰ láº¥y má»™t sá»‘ nhÃ³m chá»§ Ä‘á» Ä‘á»ƒ Ä‘Æ¡n giáº£n hÃ³a)\n",
    "categories = ['sci.space', 'comp.graphics', 'rec.sport.baseball', 'talk.politics.guns']\n",
    "train_data = fetch_20newsgroups(subset='train', categories=categories)\n",
    "test_data = fetch_20newsgroups(subset='test', categories=categories)\n",
    "\n",
    "# 2ï¸âƒ£ Táº¡o pipeline gá»“m TfidfVectorizer vÃ  mÃ´ hÃ¬nh Multinomial NaÃ¯ve Bayes\n",
    "model = make_pipeline(TfidfVectorizer(), MultinomialNB())\n",
    "\n",
    "# 3ï¸âƒ£ Huáº¥n luyá»‡n mÃ´ hÃ¬nh\n",
    "model.fit(train_data.data, train_data.target)\n",
    "\n",
    "# 4ï¸âƒ£ ÄÃ¡nh giÃ¡ mÃ´ hÃ¬nh trÃªn táº­p kiá»ƒm tra\n",
    "predicted = model.predict(test_data.data)\n",
    "accuracy = metrics.accuracy_score(test_data.target, predicted)\n",
    "print(f\"ğŸ¯ Äá»™ chÃ­nh xÃ¡c trÃªn táº­p test: {accuracy:.4f}\")\n",
    "\n",
    "# 5ï¸âƒ£ Dá»± Ä‘oÃ¡n chá»§ Ä‘á» cho má»™t vÄƒn báº£n má»›i\n",
    "new_docs = [\n",
    "    \"NASA is planning a new space mission to Mars.\",\n",
    "    \"The graphics card performance has significantly improved in recent years.\",\n",
    "    \"The baseball game last night was thrilling!\",\n",
    "    \"The new gun control laws have sparked heated debates.\"\n",
    "]\n",
    "\n",
    "predicted_labels = model.predict(new_docs)\n",
    "for doc, label in zip(new_docs, predicted_labels):\n",
    "    print(f\"ğŸ“Œ VÄƒn báº£n: {doc}\\nğŸ‘‰ Chá»§ Ä‘á» dá»± Ä‘oÃ¡n: {train_data.target_names[label]}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['positive']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "# Táº¡o pipeline\n",
    "model = make_pipeline(TfidfVectorizer(), MultinomialNB())\n",
    "\n",
    "# Dá»¯ liá»‡u huáº¥n luyá»‡n\n",
    "X_train = [\"I love Python\", \"Python is great\", \"Machine learning with Python\",\"python isbad\"]\n",
    "y_train = [\"positive\", \"positive\", \"neutral\",\"negative\"]\n",
    "\n",
    "# Huáº¥n luyá»‡n mÃ´ hÃ¬nh\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Dá»± Ä‘oÃ¡n\n",
    "X_test = [\"I hate\"]\n",
    "print(model.predict(X_test))  # Káº¿t quáº£: ['neutral']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To change the model from Multinomial Naive Bayes to Logistic Regression, you need to import `LogisticRegression` from `sklearn.linear_model` and update the pipeline accordingly. Here is the modified code:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¯ Äá»™ chÃ­nh xÃ¡c trÃªn táº­p test: 0.9514\n",
      "ğŸ“Œ VÄƒn báº£n: NASA is planning a new space mission to Mars.\n",
      "ğŸ‘‰ Chá»§ Ä‘á» dá»± Ä‘oÃ¡n: sci.space\n",
      "\n",
      "ğŸ“Œ VÄƒn báº£n: The graphics card performance has significantly improved in recent years.\n",
      "ğŸ‘‰ Chá»§ Ä‘á» dá»± Ä‘oÃ¡n: comp.graphics\n",
      "\n",
      "ğŸ“Œ VÄƒn báº£n: The baseball game last night was thrilling!\n",
      "ğŸ‘‰ Chá»§ Ä‘á» dá»± Ä‘oÃ¡n: rec.sport.baseball\n",
      "\n",
      "ğŸ“Œ VÄƒn báº£n: The new gun control laws have sparked heated debates.\n",
      "ğŸ‘‰ Chá»§ Ä‘á» dá»± Ä‘oÃ¡n: talk.politics.guns\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn import metrics\n",
    "\n",
    "# 1ï¸âƒ£ Táº£i dá»¯ liá»‡u (chá»‰ láº¥y má»™t sá»‘ nhÃ³m chá»§ Ä‘á» Ä‘á»ƒ Ä‘Æ¡n giáº£n hÃ³a)\n",
    "categories = ['sci.space', 'comp.graphics', 'rec.sport.baseball', 'talk.politics.guns']\n",
    "train_data = fetch_20newsgroups(subset='train', categories=categories)\n",
    "test_data = fetch_20newsgroups(subset='test', categories=categories)\n",
    "\n",
    "# 2ï¸âƒ£ Táº¡o pipeline gá»“m TfidfVectorizer vÃ  mÃ´ hÃ¬nh Logistic Regression\n",
    "model = make_pipeline(TfidfVectorizer(), LogisticRegression(max_iter=1000))\n",
    "\n",
    "# 3ï¸âƒ£ Huáº¥n luyá»‡n mÃ´ hÃ¬nh\n",
    "model.fit(train_data.data, train_data.target)\n",
    "\n",
    "# 4ï¸âƒ£ ÄÃ¡nh giÃ¡ mÃ´ hÃ¬nh trÃªn táº­p kiá»ƒm tra\n",
    "predicted = model.predict(test_data.data)\n",
    "accuracy = metrics.accuracy_score(test_data.target, predicted)\n",
    "print(f\"ğŸ¯ Äá»™ chÃ­nh xÃ¡c trÃªn táº­p test: {accuracy:.4f}\")\n",
    "\n",
    "# 5ï¸âƒ£ Dá»± Ä‘oÃ¡n chá»§ Ä‘á» cho má»™t vÄƒn báº£n má»›i\n",
    "new_docs = [\n",
    "    \"NASA is planning a new space mission to Mars.\",\n",
    "    \"The graphics card performance has significantly improved in recent years.\",\n",
    "    \"The baseball game last night was thrilling!\",\n",
    "    \"The new gun control laws have sparked heated debates.\"\n",
    "]\n",
    "\n",
    "predicted_labels = model.predict(new_docs)\n",
    "for doc, label in zip(new_docs, predicted_labels):\n",
    "    print(f\"ğŸ“Œ VÄƒn báº£n: {doc}\\nğŸ‘‰ Chá»§ Ä‘á» dá»± Ä‘oÃ¡n: {train_data.target_names[label]}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
