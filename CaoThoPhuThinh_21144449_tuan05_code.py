import streamlit as st
import re
from io import StringIO
# Import necessary modules
import pandas as pd
import requests
from bs4 import BeautifulSoup
from re import findall
import time
from nltk.corpus import wordnet
from nltk.tokenize import sent_tokenize
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
import contractions
from spellchecker import SpellChecker
import nltk
from nltk import pos_tag, word_tokenize, RegexpParser
from nltk import pos_tag
from nltk import word_tokenize
from nltk.stem.porter import PorterStemmer
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer
from nltk.tokenize import word_tokenize
from IPython.display import display
import string
from googletrans import Translator
import random
from langdetect import detect, detect_langs
from collections import Counter
import seaborn as sns
import spacy
import networkx as nx
# from sklearn.preprocessing import MultiLabelBinarizer
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.feature_extraction.text import TfidfVectorizer
nltk.download('punkt_tab')
nltk.download('stopwords')
nltk.download('averaged_perceptron_tagger_eng')
nltk.download('wordnet')
# import svgling # Import svgling for visualization


def remove_stopwords(text):
    stop_words = set(stopwords.words("english"))
    word_tokens = word_tokenize(text.lower())  # Chuy·ªÉn v·ªÅ ch·ªØ th∆∞·ªùng
    filtered_text = " ".join(
        word for word in word_tokens if word not in stop_words)
    return filtered_text


# def clean_text(input_text):
#     # 1. X√≥a d·∫•u ch·∫•m c√¢u X√≥a bi·ªÉu t∆∞·ª£ng (v√≠ d·ª•: @, #, $, %, ...)
#     input_text = re.sub(r'[^\w\s]', '', input_text)

#     # 2. X√≥a s·ªë
#     input_text = re.sub(r'\d+', '', input_text)

#     # 3. X√≥a kho·∫£ng tr·∫Øng d∆∞ th·ª´a
#     input_text = re.sub(r'\s+', ' ', input_text).strip()
#     return re.sub(r'[^a-zA-Z0-9\s]', '', input_text).strip()


def get_comment(soup):
    soupcmt = soup
    article_cmt = soupcmt.find_all('article', 'user-review-item')
    cmt = []
    # print(article_cmt)
    for i in article_cmt:
        try:
            # print("hi")
            title = i.find("h3", "ipc-title__text").text
            # print(title)
            content = i.find("div", "ipc-html-content-inner-div").text
            star = i.find("span", "ipc-rating-star--rating").text
            cmt.append([title, star, content])
        except:
            continue
    return cmt


def remove_numbers(text):
    result = re.sub(r'\d+', '', text)
    return result


def remove_whitespace(text):
    return " ".join(text.split())


def remove_punctuation(text):
    # translator = str.maketrans('', '', string.punctuation)
    return text.translate(str.maketrans("", "", string.punctuation))


def clean_text(text):
    if clean_lower:
        text = text.lower()
    if clean_ExpandingContractions:
        text = contractions.fix(text)
    if clean_PunctuationRemoval:
        text = text.translate(str.maketrans("", "", string.punctuation))

    if clean_numbers:
        text = re.sub(r"\d+", "", text)

    if clean_spaces:
        text = " ".join(text.split())

    if clean_StopWordsRemoval:
        # print("xoa tu dung ")
        stop_words = set(stopwords.words("english"))
        words = word_tokenize(text)
        text = " ".join(word for word in words if word not in stop_words)
        # print(text)
    if clean_symbols:
        text = re.sub(r"[^\w\s]", "", text)

    if clean_Stemming:
        stemmer = PorterStemmer()
        words = word_tokenize(text)
        text = " ".join(stemmer.stem(word) for word in words)

    if clean_Lemmatization:
        lemmatizer = WordNetLemmatizer()
        words = word_tokenize(text)
        text = " ".join(lemmatizer.lemmatize(word) for word in words)

    if clean_specialchar:
        text = re.sub(r"[^a-zA-Z0-9\s]", "", text)

    return text


st.title("X·ª≠ l√Ω VƒÉn B·∫£n")


def crawl_text(limit):
    user_agent = "checkvote"
    token="eyJhbGciOiJSUzI1NiIsImtpZCI6IlNIQTI1NjpzS3dsMnlsV0VtMjVmcXhwTU40cWY4MXE2OWFFdWFyMnpLMUdhVGxjdWNZIiwidHlwIjoiSldUIn0.eyJzdWIiOiJ1c2VyIiwiZXhwIjoxNzQxMjI5ODE2LjcyMTE2LCJpYXQiOjE3NDExNDM0MTYuNzIxMTYsImp0aSI6Ii1UTjBrQ2o3Qzd0YjdmZEdqNlFyb25BcmxMckg4USIsImNpZCI6ImRocnM3NkZIbUdDWWFGZVNWZ0FUcFEiLCJsaWQiOiJ0Ml80MGRxd29hNiIsImFpZCI6InQyXzQwZHF3b2E2IiwibGNhIjoxNTYxNDUxOTE2ODYwLCJzY3AiOiJlSnlLVnRKU2lnVUVBQURfX3dOekFTYyIsImZsbyI6OX0.HgPghVzn0jeg1I5U4gaTl5hnYceoZ-lrKDCBQpq-GPaXlEls6Z41ziYbU0hZbC-BSr3cGjw5-TJ0WdyaaFkVVJxiNCv_TkqFyHrCQDcTWmgGAqg3indBjX8YrojjiXDzAeEK1_UlDsYfHwSzxwjOkZIrR7IUjfAWXHyzMqxOG4eRVe0QCm1jvEXlgIeFaRRqNxWxk-Bvy-OT9bHlc-kCbIRTEh6QpIj-W6uAAbNFIxjryUahuFO4HWn7X_yamGednbq2OjOLDZ5_MeQx044N1Rc2HGnjiDqkB-A4aihqvygRh7BLk-FKm2RfhupaHW-6JLafbKDSLivzungWk2PgPA"
    headers = {
        "Authorization": f"bearer {token}",
        "User-Agent": user_agent
    }
    params = {"limit": limit, "t": "week"}
    response = requests.get(
        "https://oauth.reddit.com/r/shortstories/top",
        headers=headers,
        params=params,
    )
    ups = []
    text = []
    title = []
    posts = response.json()

    for i in range(len(posts["data"]["children"])):

        text.append(posts["data"]["children"][i]['data']
                    ['selftext'].replace('\r', '').replace('\n', ' '))
        ups.append(posts["data"]["children"][i]['data']['ups'])
        title.append(posts["data"]["children"][i]['data']['title'])
    data = {'title': title, 'ups': ups, 'text': text, }

    return pd.DataFrame.from_dict(data)


if "text" not in st.session_state:
    st.session_state.text = []
option = st.radio("Ch·ªçn ph∆∞∆°ng th·ª©c nh·∫≠p vƒÉn b·∫£n:", [
                  "Nh·∫≠p s·ªë ƒë·ªÉ crawl vƒÉn b·∫£n", "T·∫£i l√™n file .txt"])
if option == "T·∫£i l√™n file .txt":
    uploaded_file = st.file_uploader("Ch·ªçn t·ªáp ƒë·ªÉ t·∫£i l√™n", type=["txt"])
    if uploaded_file is not None:
        stringio = StringIO(uploaded_file.getvalue().decode("utf-8"))
        text = stringio.read()
        st.session_state.text = text
        # st.text_area("N·ªôi dung t·ªáp:", text, height=150)
elif option == "Nh·∫≠p s·ªë ƒë·ªÉ crawl vƒÉn b·∫£n":
    number = st.number_input("Nh·∫≠p m·ªôt s·ªë:", min_value=1,
                             step=1, max_value=50, value=3)
    if st.button("Crawl"):
        df = crawl_text(number)
        st.dataframe(df)
        st.session_state.df = df
        # st.text_area("VƒÉn b·∫£n ƒë√£ crawl:", str(text), height=150)

# txt_areaarea = st.text_area("Nh·∫≠p vƒÉn b·∫£n...")
st.title("B2 TƒÉng c∆∞·ªùng d·ªØ li·ªáu")


def synonym_replacement_EN(text, n=2):
    # text=df['text']
    words = word_tokenize(text)
    new_words = words.copy()
    random_words = list(set(words))
    random.shuffle(random_words)

    num_replaced = 0
    for word in random_words:
        synonyms = wordnet.synsets(word)
        if synonyms:
            synonym = synonyms[0].lemmas()[0].name()
            if synonym != word:
                new_words = [synonym if w == word else w for w in new_words]
                num_replaced += 1
            if num_replaced >= n:
                break
    return ' '.join(new_words)


def word_shuffling(text):
    words = text.split()
    random.shuffle(words)
    return ' '.join(words)


def noise_injection(text, probability=0.1):
    words = list(text)  # Chuy·ªÉn chu·ªói th√†nh danh s√°ch k√Ω t·ª±
    for i in range(len(words)):  # Duy·ªát t·ª´ng k√Ω t·ª±
        if random.random() < probability:  # X√°c su·∫•t thay th·∫ø k√Ω t·ª±
            # Thay b·∫±ng k√Ω t·ª± ng·∫´u nhi√™n
            words[i] = random.choice('abcdefghijklmnopqrstuvwxyz')
    return ''.join(words)  # Gh√©p l·∫°i th√†nh chu·ªói


def random_word_deletion(text, probability=0.2):
    words = text.split()
    new_words = [word for word in words if random.random() > probability]
    if len(new_words) == 0:  # Ensure at least one word remains
        return random.choice(words)
    return ' '.join(new_words)


async def back_translation(text, lang='fr'):

    translator = Translator()
    translated = await translator.translate(text, dest=lang)
    back_translated = await translator.translate(translated.text, src=lang, dest=detect(text))
    return back_translated.text
# def tangcuong(df, nn)
b2op = st.radio("Ch·ªçn ph∆∞∆°ng ph√°p tƒÉng c∆∞·ªùng", ["Thay th·∫ø t·ª´ ƒë·ªìng nghƒ©a",
                                                "X√°o tr·ªôn t·ª´",
                                                "Th√™m nhi·ªÖu",
                                                "X√≥a t·ª´ ng·∫´u nhi√™n"], horizontal=True)
num_row = st.number_input("Nh·∫≠p s·ªë c√¢u c·∫ßn tƒÉng c∆∞·ªùng", min_value=4)


if b2op == "Thay th·∫ø t·ª´ ƒë·ªìng nghƒ©a":
    st.write("B·∫°n ƒë√£ ch·ªçn ph∆∞∆°ng ph√°p thay th·∫ø t·ª´ ƒë·ªìng nghƒ©a.")
    # synonym_replacement_EN(st.session_state.df)
    num_dongnghia = st.number_input("so tu thay the tren moi bai", min_value=4)


elif b2op == "X√°o tr·ªôn t·ª´":
    st.write("B·∫°n ƒë√£ ch·ªçn ph∆∞∆°ng ph√°p x√°o tr·ªôn t·ª´.")
    # Th√™m logic x·ª≠ l√Ω t·∫°i ƒë√¢y

elif b2op == "Th√™m nhi·ªÖu":
    st.write("B·∫°n ƒë√£ ch·ªçn ph∆∞∆°ng ph√°p th√™m nhi·ªÖu.")
    num_xacsuat = st.number_input(
        label="Nh·∫≠p x√°c su·∫•t nhi·ªÖu", min_value=0.1, max_value=1.00, step=0.05)

elif b2op == "X√≥a t·ª´ ng·∫´u nhi√™n":
    st.write("B·∫°n ƒë√£ ch·ªçn ph∆∞∆°ng ph√°p x√≥a t·ª´ ng·∫´u nhi√™n.")
    # Th√™m logic x·ª≠ l√Ω t·∫°i ƒë√¢y
    num_xacsuat = st.number_input(
        label="Nh·∫≠p x√°c su·∫•t nhi·ªÖu", min_value=0.1, max_value=1.00, step=0.05)





b2btn = st.button("x·ª≠ l√Ω tang cuong")
if b2btn:
    if "df" not in st.session_state:
        st.error("Vui l√≤ng crawl d·ªØ li·ªáu tr∆∞·ªõc khi tƒÉng c∆∞·ªùng!")
    else:
        new_rows = []
        for i in range(num_row):
            random_num = random.randint(0, len(st.session_state.df) - 1)
            row = st.session_state.df.loc[random_num].copy()
            if b2op == "Thay th·∫ø t·ª´ ƒë·ªìng nghƒ©a":
                row['text'] = synonym_replacement_EN(row['text'], num_dongnghia)
            elif b2op == "X√°o tr·ªôn t·ª´":
                row['text'] = word_shuffling(row['text'])
            elif b2op == "Th√™m nhi·ªÖu":
                row['text'] = noise_injection(row['text'], num_xacsuat)
            elif b2op == "X√≥a t·ª´ ng·∫´u nhi√™n":
                row['text'] = random_word_deletion(row['text'], num_xacsuat)
            new_rows.append(row)
        st.session_state.df = pd.concat(
            [st.session_state.df, pd.DataFrame(new_rows)], ignore_index=True
        )
        st.dataframe(st.session_state.df)
if "df" in st.session_state :
    st.dataframe(st.session_state.df)
    st.write("So luong entry: ",len(st.session_state.df))
# txt_areaarea = st.text_area("VƒÉn b·∫£n...",st.session_state.text)
st.title("B∆∞·ªõc 3 X·ª≠ l√Ω ti·ªÅn d·ªØ li·ªáu")
col1, col2, col3 = st.columns(3)

with col1:
    clean_lower = st.checkbox("Chuy·ªÉn ƒë·ªïi ch·ªØ th∆∞·ªùng")
    clean_numbers = st.checkbox("X√≥a s·ªë")
    clean_symbols = st.checkbox("X√≥a c√°c bi·ªÉu t∆∞·ª£ng")
    clean_ExpandingContractions = st.checkbox("X·ª≠ l√Ω t·ª´ vi·∫øt t·∫Øt")

with col2:
    clean_PunctuationRemoval = st.checkbox("Lo·∫°i b·ªè d·∫•u c√¢u")
    clean_spaces = st.checkbox("Lo·∫°i b·ªè kho·∫£ng tr·∫Øng th·ª´a")
    clean_specialchar = st.checkbox("Nh·∫≠n di·ªán v√† lo·∫°i b·ªè k√Ω t·ª± ƒë·∫∑c bi·ªát")

with col3:
    clean_StopWordsRemoval = st.checkbox("X√≥a t·ª´ d·ª´ng")
    clean_Stemming = st.checkbox("C·∫Øt g·ªëc t·ª´ Stemming")
    clean_Lemmatization = st.checkbox("Chu·∫©n h√≥a t·ª´ v·ªÅ t·ª´ ƒëi·ªÉn Lemmatization")

if st.button("X·ª≠ l√Ω"):
    st.session_state.df2 = st.session_state.df.copy()
    st.session_state.df2["text"] = st.session_state.df2["text"].apply(
        clean_text)
    st.dataframe(st.session_state.df2)
if "df2" in st.session_state :
    st.dataframe(st.session_state.df2)
    st.write("So luong entry: ",len(st.session_state.df2))
st.title("B∆∞·ªõc 4 Text Representation")

option_pre = st.radio("Ch·ªçn c√°ch x·ª≠ l√Ω vƒÉn b·∫£n:",
                      ["NER ƒë·ªÉ hi·ªÉn th·ªã th·ª±c th·ªÉ",
                       "Tokenization",
                       "POS tagging",
                       "Parsing",
                       "One-Hot Encoding",
                       "Bag of Words (BoW)",
                       "Bag of N-Grams",
                       "TF-IDF"
                       ], horizontal=True)

from wordcloud import WordCloud
import matplotlib.pyplot as plt
nlp = spacy.load("en_core_web_sm")
if option_pre=="Parsing" or \
        option_pre == "NER ƒë·ªÉ hi·ªÉn th·ªã th·ª±c th·ªÉ"\
        and "df2" in st.session_state :
    selected_text = st.selectbox("Ch·ªçn b√†i ƒë·ªÉ ph√¢n t√≠ch:", st.session_state.df2["text"])
    
    # Ph√¢n t√≠ch c√∫ ph√°p v·ªõi spaCy
    if  str(selected_text) !="" :
        doc = nlp(str(selected_text))
        sentences = list(doc.sents)
        selected_sent=st.selectbox("Ch·ªçn c√¢u ƒë·ªÉ ph√¢n t√≠ch",[str(sent) for sent in sentences])
if option_pre =="One-Hot Encoding"  or option_pre =="Bag of Words (BoW)" or option_pre =="TF-IDF":
    selected_text = st.selectbox("Ch·ªçn b√†i ƒë·ªÉ ph√¢n t√≠ch:", st.session_state.df2["text"])
    if  str(selected_text) !="" :
        doc = nlp(str(selected_text))
        sentences = list(doc.sents)
elif option_pre=="Bag of N-Grams":
    selected_text = st.selectbox("Ch·ªçn b√†i ƒë·ªÉ ph√¢n t√≠ch:", st.session_state.df2["text"])
    if  str(selected_text) !="" :
        doc = nlp(str(selected_text))
        sentences = list(doc.sents)
    start_gram=st.number_input("start_gram",value=2)
    end_gram=st.number_input("end_gram",value=2)

if st.button("X·ª≠ l√Ω bi·ªÉu di·ªÖn"):
    st.session_state.df3 = st.session_state.df2.copy()

    if option_pre == "NER ƒë·ªÉ hi·ªÉn th·ªã th·ª±c th·ªÉ":
        doc_sent = nlp(str(selected_sent))

        # 1Ô∏è‚É£ T·∫°o danh s√°ch th·ª±c th·ªÉ
        entities = [{"Th·ª±c th·ªÉ": ent.text, "Lo·∫°i": ent.label_} for ent in doc_sent.ents]

        # 2Ô∏è‚É£ Chuy·ªÉn th√†nh DataFrame
        df_entities = pd.DataFrame(entities)
        if not df_entities.empty:
            st.table(df_entities)
        else:
            st.write("Kh√¥ng t√¨m th·∫•y th·ª±c th·ªÉ n√†o!")

    elif option_pre == "Tokenization":
        st.title("T·∫°o Word Cloud t·ª´ DataFrame")

        # Hi·ªÉn th·ªã DataFrame
        st.subheader("D·ªØ li·ªáu vƒÉn b·∫£n:")
        st.write(st.session_state.df3)

        # 1Ô∏è‚É£ G·ªôp to√†n b·ªô vƒÉn b·∫£n trong c·ªôt "text" th√†nh m·ªôt chu·ªói
        text_data = " ".join(st.session_state.df3["text"]).lower()  # Chuy·ªÉn v·ªÅ ch·ªØ th∆∞·ªùng

        # 2Ô∏è‚É£ Tokenization
        tokens = word_tokenize(text_data)

        # 3Ô∏è‚É£ X√≥a d·∫•u c√¢u
        tokens = [word for word in tokens if word not in string.punctuation]

        # 4Ô∏è‚É£ G·ªôp danh s√°ch tokens th√†nh chu·ªói
        processed_text = " ".join(tokens)

        # 5Ô∏è‚É£ In k·∫øt qu·∫£ ki·ªÉm tra
        # st.write("üìå VƒÉn b·∫£n sau khi x·ª≠ l√Ω:")
        # st.write(processed_text)

        # 6Ô∏è‚É£ T·∫°o & hi·ªÉn th·ªã Word Cloud
        wordcloud = WordCloud(width=800, height=400, background_color="white").generate(processed_text)

        st.subheader("Word Cloud")
        fig, ax = plt.subplots(figsize=(10, 5))
        ax.imshow(wordcloud, interpolation="bilinear")
        ax.axis("off")
        st.pyplot(fig)

    elif option_pre == "POS tagging":
        text_data = " ".join(st.session_state.df3["text"]).lower()

        # 2Ô∏è‚É£ Tokenization
        tokens = word_tokenize(text_data)

        # 3Ô∏è‚É£ POS Tagging
        pos_tags = nltk.pos_tag(tokens)

        # 4Ô∏è‚É£ ƒê·∫øm s·ªë l∆∞·ª£ng t·ª´ng lo·∫°i t·ª´
        pos_counts = Counter(tag for _, tag in pos_tags)

        # 5Ô∏è‚É£ Chuy·ªÉn d·ªØ li·ªáu th√†nh DataFrame ƒë·ªÉ v·∫Ω bi·ªÉu ƒë·ªì
        df_pos = pd.DataFrame(pos_counts.items(), columns=["POS Tag", "Count"])
        df_pos = df_pos.sort_values(by="Count", ascending=False)  # S·∫Øp x·∫øp gi·∫£m d·∫ßn

        # 6Ô∏è‚É£ V·∫Ω Bar Chart v·ªõi Seaborn
        st.subheader("Bar Chart - Ph√¢n lo·∫°i t·ª´ lo·∫°i (POS)")
        fig, ax = plt.subplots(figsize=(10, 5))
        sns.barplot(x=df_pos["POS Tag"], y=df_pos["Count"], palette="viridis", ax=ax)
        ax.set_xlabel("POS Tag")
        ax.set_ylabel("S·ªë l∆∞·ª£ng")
        ax.set_title("T·∫ßn su·∫•t c√°c t·ª´ lo·∫°i trong vƒÉn b·∫£n")
        st.pyplot(fig)

        # # 7Ô∏è‚É£ Hi·ªÉn th·ªã d·ªØ li·ªáu POS ƒë·ªÉ ki·ªÉm tra
        # st.subheader("Chi ti·∫øt POS Tagging")
        # st.write(pos_tags)

    elif option_pre == "Parsing":
   
        
        
        doc_sent = nlp(str(selected_sent))
        # 1Ô∏è‚É£ T·∫°o ƒë·ªì th·ªã
        G = nx.DiGraph()

        # 2Ô∏è‚É£ Th√™m c√°c t·ª´ v√†o ƒë·ªì th·ªã
        for token in doc_sent:
            G.add_edge(token.head.text, token.text, label=token.dep_)

        # 3Ô∏è‚É£ V·∫Ω ƒë·ªì th·ªã
        st.subheader("Graph Network - C√¢y C√∫ Ph√°p")

        fig, ax = plt.subplots(figsize=(10, 10))
        pos = nx.spring_layout(G)  # B·ªë c·ª•c c·ªßa ƒë·ªì th·ªã
        nx.draw(G, pos, with_labels=True, node_color="lightblue", edge_color="gray", node_size=2000, font_size=10, font_weight="bold", ax=ax)

        # Hi·ªÉn th·ªã nh√£n dependency tr√™n c√°c c·∫°nh
        edge_labels = {(token.head.text, token.text): token.dep_ for token in doc_sent}
        nx.draw_networkx_edge_labels(G, pos, edge_labels=edge_labels, font_color="red", ax=ax)

        # Hi·ªÉn th·ªã ƒë·ªì th·ªã tr√™n Streamlit
        st.pyplot(fig)

        # Hi·ªÉn th·ªã chi ti·∫øt dependency tree
        st.subheader("Chi ti·∫øt Dependency Parsing:")
        dic={'T·ª´ hi·ªán t·∫°i trong c√¢u':[token.text for token in doc_sent],
             'Quan h·ªá c√∫ ph√°p':[token.dep_ for token in doc_sent],
             'T·ª´ g·ªëc':[token.head.text for token in doc_sent],
             }
        st.dataframe(pd.DataFrame.from_dict(dic))
        # for token in doc_sent:
        #     st.write(f"**{token.text}** ‚Üê ({token.dep_}) ‚Üê **{token.head.text}**")

    elif option_pre=="One-Hot Encoding":
        count_vect = CountVectorizer(binary=True)
        bow_rep = count_vect.fit_transform([str(span) for span in sentences])
        # print (bow_rep.toarray())
        # print(count_vect.get_feature_names_out())
        df = pd.DataFrame(bow_rep.toarray(), columns=count_vect.get_feature_names_out(),index=[str(span) for span in sentences])
        st.dataframe(df)
    
    elif option_pre=="Bag of Words (BoW)":
        count_vect = CountVectorizer()
        bow_rep = count_vect.fit_transform([str(span) for span in sentences])

        df3 = pd.DataFrame(bow_rep.toarray(), columns=count_vect.get_feature_names_out(),index=[str(span) for span in sentences])
        st.dataframe(df3)
    elif option_pre=="Bag of N-Grams":
        count_vect = CountVectorizer(ngram_range=(start_gram, end_gram))
        bow_rep = count_vect.fit_transform([str(span) for span in sentences])
        df3 = pd.DataFrame(bow_rep.toarray(), columns=count_vect.get_feature_names_out(), index=[str(span) for span in sentences])
        st.dataframe(df3)
    elif option_pre=="TF-IDF":
        count_vect = TfidfVectorizer()
        bow_rep = count_vect.fit_transform([str(span) for span in sentences])

        df3 = pd.DataFrame(bow_rep.toarray(), columns=count_vect.get_feature_names_out(),index=[str(span) for span in sentences])
        st.dataframe(df3)
    # result = st.text_area(label="ket qua", value=t)


